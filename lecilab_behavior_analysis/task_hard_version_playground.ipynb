{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lecilab_behavior_analysis.utils as utils\n",
    "import lecilab_behavior_analysis.plots as plots\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lecilab_behavior_analysis.df_transforms as dft\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from cluster\n",
    "tv_projects = utils.get_server_projects()\n",
    "print(tv_projects)\n",
    "# see the available animals\n",
    "animals = utils.get_animals_in_project(tv_projects[1])\n",
    "print(animals)\n",
    "# download the data for a specific animal\n",
    "mouse = \"ACV002\"\n",
    "local_path = Path(utils.get_outpath()) / Path(tv_projects[1]) / Path(\"sessions\") / Path(mouse)\n",
    "# create the directory if it doesn't exist\n",
    "local_path.mkdir(parents=True, exist_ok=True)\n",
    "# download the session data\n",
    "utils.rsync_session_data(\n",
    "    project_name=tv_projects[1],\n",
    "    animal=mouse,\n",
    "    local_path=str(local_path),\n",
    "    credentials=utils.get_idibaps_cluster_credentials(),\n",
    ")\n",
    "# load the data\n",
    "df = pd.read_csv(local_path / Path(f'{mouse}.csv'), sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the dataset to the psychometric version of the task\n",
    "# Otherwise, we would include a lot of \"easy\" trials that would bias the fit\n",
    "df_test = df[df[\"current_training_stage\"] == \"TwoAFC_visual_hard\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "psychometric curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = dft.get_performance_by_difficulty_ratio(df_test)\n",
    "plots.psychometric_plot(df_test, x = 'visual_stimulus_ratio', y = 'first_choice_numeric')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = dft.get_performance_by_difficulty_diff(df_test)\n",
    "plots.psychometric_plot(df_test, x = 'visual_stimulus_diff', y = 'first_choice_numeric', valueType = 'continue', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLM comparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following cell can be use to evaluate the model. It will be useful when comparing different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Define the lapse logistic function with independent lapses for left and right\n",
    "def lapse_logistic_independent(params, x, y):\n",
    "    lapse_left, lapse_right, beta, x0 = params\n",
    "    # Ensure lapse rates are within [0, 0.5]\n",
    "    lapse_left = np.clip(lapse_left, 0, 0.5)\n",
    "    lapse_right = np.clip(lapse_right, 0, 0.5)\n",
    "    # Predicted probabilities\n",
    "    p_left = lapse_left + (1 - lapse_left - lapse_right) / (1 + np.exp(-beta * (x - x0)))\n",
    "    # Negative log-likelihood\n",
    "    nll = -np.sum(y * np.log(p_left) + (1 - y) * np.log(1 - p_left))\n",
    "    return nll\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "log_losses = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kf.split(df_test):\n",
    "    # Split the data\n",
    "    x_train, x_test = df_test['visual_stimulus_ratio'].values[train_index], df_test['visual_stimulus_ratio'].values[test_index]\n",
    "    y_train, y_test = df_test['first_choice_numeric'].values[train_index], df_test['first_choice_numeric'].values[test_index]\n",
    "    \n",
    "    # Initial parameter guesses: [lapse_left, lapse_right, beta, x0]\n",
    "    initial_params = [0.05, 0.05, 1, 0]\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    result = minimize(\n",
    "        lapse_logistic_independent,\n",
    "        initial_params,\n",
    "        args=(x_train, y_train),\n",
    "        bounds=[(0, 0.5), (0, 0.5), (None, None), (None, None)]\n",
    "    )\n",
    "    \n",
    "    # Extract fitted parameters\n",
    "    lapse_left, lapse_right, beta, x0 = result.x\n",
    "    \n",
    "    # Generate predictions on the test data\n",
    "    p_left_test = lapse_left + (1 - lapse_left - lapse_right) / (1 + np.exp(-beta * (x_test - x0)))\n",
    "    \n",
    "    # Calculate log loss for the test data\n",
    "    loss = log_loss(y_test, p_left_test)\n",
    "    log_losses.append(loss)\n",
    "\n",
    "# Print cross-validation results\n",
    "print(f\"Cross-Validation Log Losses: {log_losses}\")\n",
    "print(f\"Mean Log Loss: {np.mean(log_losses)}\")\n",
    "print(f\"Standard ratio of Log Loss: {np.std(log_losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight and stats for the different predictors:\n",
    "- visual stimulus ratio (you call it deviation)\n",
    "- visual stimulus diff. Nuo: change to \"total intensity on left port\"\n",
    "- port where the animal is coming from\n",
    "- interactions\n",
    "- Nuo: add another regressor: the previous correct choice\n",
    "\n",
    "We can play around with this things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['visual_stimulus_ratio',\n",
    "    'previous_port_before_stimulus_numeric',\n",
    "    'visual_ratio_diff_interact',\n",
    "    'previous_left_choice_correct_numeric',\n",
    "    'previous_right_choice_wrong_numeric',\n",
    "    'previous_first_choice_numeric', \n",
    "    'visual_ratio_bright_interact', \n",
    "    'previous_last_choice_numeric'\n",
    "        ]\n",
    "df_new_for_fit = dft.parameters_for_fit(df_test)\n",
    "results, model = utils.logi_model_fit(df_new_for_fit, X = X, y = 'first_choice_numeric')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correct choice as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_for_fit = dft.parameters_for_fit(df_test)\n",
    "results = utils.logi_model_fit(df_new_for_fit, X = ['visual_stimulus_ratio',\n",
    "                                                    'wrong_bright', \n",
    "                                                    # 'wrong_bright_zscore',\n",
    "                                                    'previous_same_choice_correct_numeric', \n",
    "                                                    # 'previous_diff_choice_wrong_numeric', \n",
    "                                                    'previous_same_choice_numeric', \n",
    "                                                    'previous_correct_numeric'\n",
    "                                                    ], y = 'correct_numeric')\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_impact_on_time_kernel(series, max_lag=10, tau=5):\n",
    "    kernel = np.exp(np.arange(1, max_lag+1) / tau)\n",
    "    padded = np.concatenate([[0]*len(kernel), series])\n",
    "    # time kernel convolve\n",
    "    return np.array([\n",
    "        np.dot(kernel, padded[i:i+len(kernel)])\n",
    "        for i in range(len(series))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The kernel stimates the weight subjects gives to each stimulus frame. It's usually computed via logistic regression\n",
    "(https://en.wikipedia.org/wiki/Logistic_regression). We estimate the probability of a decision 'right' given some filters\n",
    "(the betas or weights).\n",
    "- p is the probability of choose right\n",
    "- B0 isn't multiplied by any x and therefore is the bias. Normally is not included, but if the subject is biased, it's\n",
    "best to do so. Bi are the weights of each frame, and there's one beta for each x\n",
    "- x are the frames, there's one x for each B\n",
    "\n",
    "In the wikipedia example plot, the x-axis would be the stimulus strength and the y-axis would be probability of\n",
    "choose right. Then we fit the logistic regression curve. When we plot a kernel, what we're actually representing are\n",
    "values of Bi. The values of beta can be computed in python with the 'logistic regression' from the 'sklearn' library\n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "- x is a matrix with my stimulus strengths (1 row per stimulus, one column for each frame, so 1*10)\n",
    "- y is a vector with the subjects' choices\n",
    "\"\"\"\n",
    "endog = choices  # Your regressand (y)\n",
    "exog = stimuli_frames (evidences)  # Your regressors (x)\n",
    "model = sm.GLM(endog, exog, family=sm.families.Binomial(), missing='drop')  # GLM with Binomial family\n",
    "results = model.fit()\n",
    "params = results.params\n",
    "beta_std_err = results.bse\n",
    "p_values = results.pvalues\n",
    "summary = results.summary()\n",
    "print(summary)\n",
    "\n",
    "plt.plot(params)  # The so-called kernel is just the plot of this weights (params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "auditory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auditory = df[df['current_training_stage'] == 'TwoAFC_auditory_hard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([get_timebin_evidence(eval(t)) for t in df_auditory['auditory_stimulus']])\n",
    "df_aud_fit = dft.parameters_for_fit(df_auditory)\n",
    "y = df_aud_fit['first_choice_numeric']\n",
    "X_model = sm.add_constant(X) \n",
    "glm = sm.Logit(y, X_model).fit()\n",
    "plt.plot(glm.params[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auditory = dft.get_performance_by_difficulty_ratio(df_auditory)\n",
    "plots.psychometric_plot(df_auditory, x = 'total_evidence_strength', y = 'first_choice_numeric', valueType = 'continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auditory_fit = dft.parameters_for_fit(df_auditory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, _ = utils.logi_model_fit(df_auditory_fit, X = ['total_percentage_of_tones_left',\n",
    "                      'number_of_tones_left',\n",
    "                      'percentage_of_timebins_with_evidence_left', \n",
    "                      'total_evidence_strength', \n",
    "                      'amplitude_strength'\n",
    "                          ], y = 'first_choice_numeric')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct wrong psychometric curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "for i, linecolor in zip(df_new_for_fit[df_new_for_fit['previous_first_choice_numeric'] == 1].groupby('previous_correct_numeric'), ['red', 'green']):\n",
    "    plots.psychometric_plot(df = i[1], \n",
    "                            x = 'visual_stimulus_ratio', \n",
    "                            y = 'first_choice_numeric', \n",
    "                            ax=ax[0],\n",
    "                            point_kwargs={'marker': 'o', 'color': 'k', 'label': ''},\n",
    "                            line_kwargs={'color': linecolor, 'label': 'previous ' + str(i[0])}\n",
    "                                        )\n",
    "\n",
    "for i, linecolor in zip(df_new_for_fit[df_new_for_fit['previous_first_choice_numeric'] == 0].groupby('previous_correct_numeric'), ['red', 'green']):\n",
    "    plots.psychometric_plot(df = i[1], \n",
    "                            x = 'visual_stimulus_ratio', \n",
    "                            y = 'first_choice_numeric', \n",
    "                            ax=ax[1],\n",
    "                            point_kwargs={'marker': 'o', 'color': 'k', 'label': ''},\n",
    "                            line_kwargs={'color': linecolor, 'label': 'previous ' + str(i[0])}\n",
    "                                        )\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"Left Choice Previous\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"Right Choice Previous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "for i, linecolor in zip(df_new_for_fit[df_new_for_fit['previous_correct_numeric'] == True].groupby('previous_first_choice_numeric'), ['gold', 'lightskyblue']):\n",
    "    plots.psychometric_plot(df = i[1], \n",
    "                            x = 'visual_stimulus_ratio', \n",
    "                            y = 'first_choice_numeric', \n",
    "                            ax=ax[0],\n",
    "                            point_kwargs={'marker': 'o', 'color': 'k', 'label': ''},\n",
    "                            line_kwargs={'color': linecolor, 'label': 'previous ' + str(i[0])}\n",
    "                        )\n",
    "\n",
    "for i, linecolor in zip(df_new_for_fit[df_new_for_fit['previous_correct_numeric'] == False].groupby('previous_first_choice_numeric'), ['gold', 'lightskyblue']):\n",
    "    plots.psychometric_plot(df = i[1], \n",
    "                            x = 'visual_stimulus_ratio', \n",
    "                            y = 'first_choice_numeric', \n",
    "                            ax=ax[1],\n",
    "                            point_kwargs={'marker': 'o', 'color': 'k', 'label': ''},\n",
    "                            line_kwargs={'color': linecolor, 'label': 'previous ' + str(i[0])}\n",
    "                        )\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"Correct Choice Previous\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"Incorrect Choice Previous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the absolute value of the lowest visual stimulus as a proxy for the brightness of the visual stimulus\n",
    "df_test['visual_stimulus_lowest'] = df_test['visual_stimulus'].apply(lambda x: abs(eval(x)[0]) if eval(x)[0] < eval(x)[1] else abs(eval(x)[1]))\n",
    "# create 10 bins for the absolute value of the lowest visual stimulus\n",
    "min_value = df_test['visual_stimulus_lowest'].min()\n",
    "max_value = df_test['visual_stimulus_lowest'].max()\n",
    "bins = np.linspace(min_value, max_value, 11)\n",
    "df_test['visual_stimulus_lowest_binned'] = pd.cut(df_test['visual_stimulus_lowest'], bins=bins, labels=[f\"{b:.2f}\" for b in bins[:-1]])\n",
    "# create a pivot table with the visual stimulus ratio and absolute value of the lowest visual stimulus\n",
    "pivot_table_abs = df_test.pivot_table(\n",
    "    index='visual_stimulus_lowest_binned',\n",
    "    columns='visual_stimulus_ratio',\n",
    "    values='first_choice_numeric',\n",
    "    aggfunc='mean',\n",
    "    observed=True\n",
    ")\n",
    "# plot the heatmap\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(pivot_table_abs, cmap='coolwarm', annot=True, fmt=\".2f\", cbar_kws={'label': 'Probability of Left Choice'})\n",
    "plt.xlabel(\"Visual Stimulus ratio\")\n",
    "plt.ylabel(\"Absolute Value of Lowest Visual Stimulus\")\n",
    "plt.title(\"Heatmap of Probability of Left Choice\")\n",
    "# rotate the y-axis labels\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform visual_stimulus_lowest_binned to a numeric value for plotting\n",
    "df_test['visual_stimulus_lowest_binned_num'] = pd.to_numeric(df_test['visual_stimulus_lowest_binned'], errors='coerce')\n",
    "\n",
    "# make two plots, one for when the animals comes from the left and one for when it comes from the right\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "# Plot for when the animal comes from the left\n",
    "for ax, side in zip(axs.ravel(), ['left', 'right']):\n",
    "    df_side = df_test[df_test['previous_port_before_stimulus'] == side]\n",
    "    for i in df_side.groupby('visual_stimulus_ratio'):\n",
    "        df_i = i[1].sort_values(by='visual_stimulus_lowest_binned_num')\n",
    "        # drop nan\n",
    "        df_i = df_i.dropna(subset=['visual_stimulus_lowest_binned_num'])\n",
    "        X = df_i['visual_stimulus_lowest_binned_num'].values.reshape(-1, 1)\n",
    "        y = df_i['first_choice_numeric'].values.astype(int)\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "        ax.plot(X, y_prob, label=f\"Visual Stimulus ratio: {i[0]}\")\n",
    "    ax.set_xlabel(\"Absolute Value of Lowest Visual Stimulus\")\n",
    "    ax.set_ylabel(\"Probability of Left Choice\")\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"Last Choice Before Stimulus: {side.capitalize()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the lapse model independently considering previous choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "colors = [\"blue\", \"orange\"]\n",
    "\n",
    "for color, side in zip(colors, ['left', 'right']):\n",
    "    df_side = df_test[df_test['previous_port_before_stimulus'] == side]\n",
    "    # Fit the model\n",
    "    x = df_side['visual_stimulus_ratio'].values\n",
    "    y = df_side['first_choice_numeric'].values\n",
    "    result = minimize(\n",
    "        lapse_logistic_independent,\n",
    "        initial_params,\n",
    "        args=(x, y),\n",
    "        bounds=[(0, 0.5), (0, 0.5), (None, None), (None, None)]\n",
    "    )\n",
    "\n",
    "    # Extract fitted parameters\n",
    "    lapse_left, lapse_right, beta, x0 = result.x\n",
    "    print(f\"Side: {side}, Lapse Left: {lapse_left}, Lapse Right: {lapse_right}, Slope (Beta): {beta}, PSE (x0): {x0}\")\n",
    "\n",
    "    # Generate predictions\n",
    "    xs = np.linspace(df_side['visual_stimulus_ratio'].min(), df_side['visual_stimulus_ratio'].max(), 100)\n",
    "    p_left = lapse_left + (1 - lapse_left - lapse_right) / (1 + np.exp(-beta * (xs - x0)))\n",
    "\n",
    "    # Plot the fitted curve\n",
    "\n",
    "    sns.pointplot(\n",
    "        x='visual_stimulus_ratio',\n",
    "        y='first_choice_numeric',\n",
    "        data=df_side,\n",
    "        estimator=lambda x: np.mean(x),\n",
    "        color=color,\n",
    "        markers='o',\n",
    "        errorbar=(\"ci\", 95),\n",
    "        ax=ax,\n",
    "        label=f'Choices when coming from {side}',\n",
    "        native_scale=True,\n",
    "        linestyles='',\n",
    "    )\n",
    "    ax.plot(xs, p_left, color=color, label='Lapse Logistic Fit')\n",
    "    ax.set_xlabel(\"Visual Stimulus ratio\")\n",
    "    ax.set_ylabel(\"Probability of Left Choice\")\n",
    "    plt.title(f\"Psychometric Curves\")\n",
    "    ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I kept what you did for comparison here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is interesting to compare the effects of the relative difference between the two visual stimuli,\n",
    "# and the absolute difference between them.\n",
    "\n",
    "# Maybe what we can do is to train another logistic regression model, adding as well the absolute difference\n",
    "# between the two visual stimuli, and see how it affects the probability of a left choice.\n",
    "# Do you know what I mean?\n",
    "\n",
    "for i in df_test.groupby('visual_stimulus_ratio'):\n",
    "    df_i = i[1].sort_values(by='visual_stimulus_diff')\n",
    "    X = df_i['visual_stimulus_diff'].values.reshape(-1, 1)\n",
    "    y = df_i['first_choice_numeric'].values.astype(int)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "    plt.plot(X, y_prob, label=f\"Visual Stimulus ratio: {i[0]}\")\n",
    "    plt.legend()\n",
    "plt.xlabel(\"Visual Stimulus Difference\")\n",
    "plt.ylabel(\"Probability of Left Choice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple animals analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data extration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic = {}\n",
    "for mouse in animals:\n",
    "    local_path = Path(utils.get_outpath()) / Path(tv_projects[1]) / Path(\"sessions\") / Path(mouse)\n",
    "    # create the directory if it doesn't exist\n",
    "    local_path.mkdir(parents=True, exist_ok=True)\n",
    "    # download the session data\n",
    "    utils.rsync_session_data(\n",
    "        project_name=tv_projects[1],\n",
    "        animal=mouse,\n",
    "        local_path=str(local_path),\n",
    "        credentials=utils.get_idibaps_cluster_credentials(),\n",
    "    )\n",
    "    # load the data\n",
    "    df_dic[mouse] = pd.read_csv(local_path / Path(f'{mouse}.csv'), sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic_hard = {}\n",
    "for df_name, df in zip(df_dic.keys(), df_dic.values()):\n",
    "    if 'TwoAFC_visual_hard' in df[\"current_training_stage\"].unique():\n",
    "        df = df[df[\"current_training_stage\"] == \"TwoAFC_visual_hard\"]\n",
    "        df = dft.get_performance_by_difficulty_ratio(df)\n",
    "        df = dft.get_performance_by_difficulty_diff(df)\n",
    "        df_dic_hard[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic_hard_fit = {}\n",
    "for df_name, df in zip(df_dic_hard.keys(), df_dic_hard.values()):\n",
    "    df_hard_fit = df.copy(deep=True)\n",
    "    df_hard_fit = dft.parameters_for_fit(df_hard_fit)\n",
    "    df_dic_hard_fit[df_name] = df_hard_fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic_hard_fit_firHalf = {}\n",
    "for df_name, df in zip(df_dic_hard_fit.keys(), df_dic_hard_fit.values()):\n",
    "    df_hard_fit_firHalf = df[:(len(df)//2)]\n",
    "    df_dic_hard_fit_firHalf[df_name] = df_hard_fit_firHalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic_hard_aud = {}\n",
    "for df_name, df in zip(df_dic.keys(), df_dic.values()):\n",
    "    if 'TwoAFC_auditory_hard' in df[\"current_training_stage\"].unique():\n",
    "        df = df[df[\"current_training_stage\"] == \"TwoAFC_auditory_hard\"]\n",
    "        df = dft.get_performance_by_difficulty_ratio(df)\n",
    "        df_dic_hard_aud[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic_hard_aud_fit = {}\n",
    "for df_name, df in zip(df_dic_hard_aud.keys(), df_dic_hard_aud.values()):\n",
    "    df_hard_aud_fit = df.copy(deep=True)\n",
    "    df_hard_aud_fit = dft.parameters_for_fit(df_hard_aud_fit)\n",
    "    df_dic_hard_aud_fit[df_name] = df_hard_aud_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "for df_name, df, color in zip(df_dic_hard.keys(), df_dic_hard.values(), sns.color_palette(\"colorblind\", len(df_dic_hard))):\n",
    "    plots.psychometric_plot(df, x='visual_stimulus_ratio', y='first_choice_numeric', point_kwargs={'color': color, 'label' : ''}, line_kwargs={'color': color, 'label': df_name})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "for df_name, df, color in zip(df_dic_hard_aud.keys(), df_dic_hard_aud.values(), sns.color_palette(\"colorblind\", len(df_dic_hard_aud))):\n",
    "    plots.psychometric_plot(df, x='total_evidence_strength', y='first_choice_numeric', valueType = 'continue', point_kwargs={'color': color, 'label' : ''}, line_kwargs={'color': color, 'label': df_name})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['visual_stimulus_ratio',\n",
    "    'visual_ratio_diff_interact',\n",
    "    'visual_ratio_bright_interact', \n",
    "    'previous_left_choice_correct_numeric',\n",
    "    'previous_right_choice_wrong_numeric',\n",
    "    'previous_first_choice_numeric', \n",
    "    'previous_last_choice_numeric', \n",
    "    'previous_port_before_stimulus_numeric',\n",
    "    # 'time_kernel_impact'\n",
    "        ]\n",
    "y = 'first_choice_numeric'\n",
    "fig, ax = plt.subplots(2, 5, figsize=(20, 10))\n",
    "for var, n in zip(X, range(len(X))):\n",
    "    row = n // 5\n",
    "    col = n % 5\n",
    "    for df_name, df, color in zip(df_dic_hard_fit.keys(), df_dic_hard_fit.values(), sns.color_palette(\"colorblind\", len(df_dic_hard_fit))):\n",
    "        df_new = df.dropna(subset=[var, y]) \n",
    "        # Check if the variable in daraframe is discrete or continuous\n",
    "        if df_new[var].nunique() < 10:\n",
    "            # If discrete, plot as discrete\n",
    "            plots.psychometric_plot(df = df_new,\n",
    "                                x = var, \n",
    "                                y = y, \n",
    "                                ax = ax[row][col],\n",
    "                                point_kwargs={'color': color, 'label': ''},\n",
    "                                line_kwargs={'color': color, 'label': df_name}, \n",
    "                                valueType='discrete'\n",
    "                                        )\n",
    "        else:\n",
    "            # If continuous, plot as continuous\n",
    "            plots.psychometric_plot(df = df_new,\n",
    "                                x = var, \n",
    "                                y = y, \n",
    "                                ax = ax[row][col],\n",
    "                                point_kwargs={'color': color, 'label': ''},\n",
    "                                line_kwargs={'color': color, 'label': df_name}, \n",
    "                                valueType='continue'\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find the optimal parameters of time kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_dict = utils.verify_params_time_kernel(dic = df_dic_hard, y='first_choice_numeric')\n",
    "sorted_items = sorted(comb_dict.items(), key=lambda item: abs(item[1]), reverse=True)\n",
    "sorted_items[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter the correlated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['visual_stimulus_ratio',\n",
    "    'visual_ratio_diff_interact',\n",
    "    'visual_ratio_bright_interact', \n",
    "    # 'previous_left_choice_correct_numeric',\n",
    "    # 'previous_right_choice_wrong_numeric',\n",
    "    # 'previous_first_choice_numeric', \n",
    "    # 'previous_last_choice_numeric', \n",
    "    # 'previous_port_before_stimulus_numeric',\n",
    "    # 'time_kernel_impact'\n",
    "        ]\n",
    "corr_mat_list, norm_contribution_df = utils.filter_variables_for_model(dic_fit=df_dic_hard_fit_firHalf, X = X, y='first_choice_numeric', max_lag=19, tau=1)\n",
    "plots.plot_filter_model_variables(corr_mat_list=corr_mat_list, norm_contribution_df=norm_contribution_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit the model by filtered variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = ['visual_stimulus_ratio',\n",
    "    'previous_port_before_stimulus_numeric',\n",
    "    'visual_ratio_diff_interact',\n",
    "    'previous_left_choice_correct_numeric',\n",
    "    # 'previous_right_choice_wrong_numeric',\n",
    "    # 'previous_first_choice_numeric', \n",
    "    'visual_ratio_bright_interact', \n",
    "    # 'previous_last_choice_numeric', \n",
    "    'time_kernel_impact'\n",
    "        ]\n",
    "df_leftChoice_model_p = pd.DataFrame()\n",
    "df_leftChoice_model_coef = pd.DataFrame()\n",
    "df_leftChoice_model_z =  pd.DataFrame()\n",
    "for df_name, df, color in zip(df_dic_hard_fit_firHalf.keys(), df_dic_hard_fit_firHalf.values(), sns.color_palette(\"colorblind\", len(df_dic_hard_fit_firHalf))):\n",
    "    df_for_fit = dft.parameters_for_fit(df)\n",
    "    df_for_fit = dft.get_time_kernel_impact(df_for_fit, y='first_choice_numeric', max_lag=19, tau=1)\n",
    "    _, model = utils.logi_model_fit(df_for_fit, X=X, y='first_choice_numeric')\n",
    "    # plot the results\n",
    "    df_leftChoice_model_p[df_name] = model.pvalues\n",
    "    df_leftChoice_model_coef[df_name] = model.params\n",
    "    df_leftChoice_model_z[df_name] = model.tvalues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare the parameters of model by p, coef, z directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the p-values coefficients and z\n",
    "fig, ax = plt.subplots(3, 1, figsize=(15, 10))\n",
    "df_leftChoice_model_p.index = ['intercept'] + X\n",
    "df_leftChoice_model_coef.index = ['intercept'] + X\n",
    "df_leftChoice_model_z.index = ['intercept'] + X\n",
    "df_leftChoice_model_p_long = df_leftChoice_model_p.reset_index().melt(id_vars='index', var_name='Mouse', value_name='Value')\n",
    "sns.boxplot(data=df_leftChoice_model_p_long, x='index', y='Value', ax=ax[0], color='lightgrey', boxprops=dict(alpha=0.3))\n",
    "sns.scatterplot(data=df_leftChoice_model_p_long, ax=ax[0], x='index', y='Value', hue='Mouse', palette='colorblind')\n",
    "ax[0].axhline(y=0.05, color='red', linestyle='--', label='Significance Threshold (0.05)')\n",
    "df_leftChoice_model_coef_long = df_leftChoice_model_coef.reset_index().melt(id_vars='index', var_name='Mouse', value_name='Value')\n",
    "sns.boxplot(data=df_leftChoice_model_coef_long, x='index', y='Value', ax=ax[1], color='lightgrey', boxprops=dict(alpha=0.3))\n",
    "sns.scatterplot(data=df_leftChoice_model_coef_long, ax=ax[1], x='index', y='Value', hue='Mouse', palette='colorblind')\n",
    "df_leftChoice_model_z_long = df_leftChoice_model_z.reset_index().melt(id_vars='index', var_name='Mouse', value_name='Value')\n",
    "sns.boxplot(data=df_leftChoice_model_z_long, x='index', y='Value', ax=ax[2], color='lightgrey', boxprops=dict(alpha=0.3))\n",
    "sns.scatterplot(data=df_leftChoice_model_z_long, ax=ax[2], x='index', y='Value', hue='Mouse', palette='colorblind')\n",
    "# make x label not overlapping\n",
    "ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=8, ha='right')\n",
    "ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=8, ha='right')\n",
    "ax[2].set_xticklabels(ax[2].get_xticklabels(), rotation=8, ha='right')\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[1].set_xlabel(\"\")\n",
    "ax[2].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"P-values\")\n",
    "ax[1].set_ylabel(\"Coefficients\")\n",
    "ax[2].set_ylabel(\"Z-scores\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correct choice model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_dict = utils.verify_params_time_kernel(dic = df_dic_hard, y='correct_numeric')\n",
    "sorted_items = sorted(comb_dict.items(), key=lambda item: abs(item[1]), reverse=True)\n",
    "sorted_items[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['abs_visual_stimulus_ratio',\n",
    "    'wrong_bright', \n",
    "    'previous_same_choice_correct_numeric', \n",
    "    'previous_same_choice_numeric', \n",
    "    'previous_correct_numeric', \n",
    "    'previous_port_before_stimulus_numeric', \n",
    "    'previous_first_choice_numeric', \n",
    "    'previous_last_choice_numeric', \n",
    "    'time_kernel_impact'\n",
    "        ]\n",
    "corr_mat_list, norm_contribution_df = utils.filter_variables_for_model(dic_fit=df_dic_hard_fit_firHalf, X = X, y='correct_numeric', max_lag=19, tau=7)\n",
    "plots.plot_filter_model_variables(corr_mat_list=corr_mat_list, norm_contribution_df=norm_contribution_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_one_var_contribution(df, x_cols, y_col, method='newton'):\n",
    "    contributions = {var: [] for var in x_cols}\n",
    "    X, y = utils.logi_model_fit_input(df, x_cols, y_col)\n",
    "    model = sm.Logit(y, X).fit(method=method)\n",
    "    r2 = r2_score(y, model.predict(X))\n",
    "    for var in x_cols:\n",
    "        X_reduced = X.drop(columns=[var])\n",
    "        model_reduced = sm.Logit(y, X_reduced).fit(method=method)\n",
    "        r2_reduced = r2_score(y, model_reduced.predict(X_reduced))\n",
    "        delta = r2 - r2_reduced\n",
    "        contributions[var].append(delta)\n",
    "    # normalize the contributions\n",
    "    avg_contrib = {var: np.mean(contrib) for var, contrib in contributions.items()}\n",
    "    total = sum(avg_contrib.values())\n",
    "    norm_contrib = {var: val / total for var, val in avg_contrib.items()}\n",
    "    return pd.Series(norm_contrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_variables_for_model_2(dic_fit:dict, X:list, y:str, max_lag=None, tau=None):\n",
    "    corr_mat_list = []\n",
    "    norm_contribution_df = pd.DataFrame([])\n",
    "    for df_name, df_for_fit in zip(dic_fit.keys(), dic_fit.values()):\n",
    "        if (max_lag is not None) & (tau is not None):\n",
    "            df_for_fit = dft.get_time_kernel_impact(df_for_fit, y=y, max_lag=max_lag, tau=tau)\n",
    "        \n",
    "        corr_fit_X_df = df_for_fit[X].corr()\n",
    "        corr_mat_list.append(corr_fit_X_df)\n",
    "\n",
    "        norm_contribution = drop_one_var_contribution(df_for_fit, x_cols = X, y_col = y, method='bfgs')\n",
    "        norm_contribution_df[df_name] = norm_contribution\n",
    "\n",
    "    return corr_mat_list, norm_contribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['abs_visual_stimulus_ratio',\n",
    "    'wrong_bright', \n",
    "    'previous_same_choice_correct_numeric', \n",
    "    'previous_same_choice_numeric', \n",
    "    'previous_correct_numeric', \n",
    "    'previous_port_before_stimulus_numeric', \n",
    "    'previous_first_choice_numeric', \n",
    "    'previous_last_choice_numeric', \n",
    "    'time_kernel_impact'\n",
    "        ]\n",
    "corr_mat_list, norm_contribution_df = filter_variables_for_model_2(dic_fit=df_dic_hard_fit_firHalf, X = X, y='correct_numeric', max_lag=19, tau=7)\n",
    "\n",
    "plots.plot_filter_model_variables(corr_mat_list=corr_mat_list, norm_contribution_df=norm_contribution_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['abs_visual_stimulus_ratio',\n",
    "    'wrong_bright', \n",
    "    'previous_same_choice_correct_numeric', \n",
    "    'previous_same_choice_numeric', \n",
    "    'previous_correct_numeric', \n",
    "    # 'previous_port_before_stimulus_numeric', \n",
    "    # 'previous_first_choice_numeric', \n",
    "    # 'previous_last_choice_numeric', \n",
    "    'time_kernel_impact'\n",
    "        ]\n",
    "corr_mat_list, norm_contribution_df = filter_variables_for_model_2(dic_fit=df_dic_hard_fit_firHalf, X = X, y='correct_numeric', max_lag=19, tau=7)\n",
    "\n",
    "plots.plot_filter_model_variables(corr_mat_list=corr_mat_list, norm_contribution_df=norm_contribution_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['visual_stimulus_ratio',\n",
    "    'wrong_bright', \n",
    "    'previous_same_choice_correct_numeric', \n",
    "    'previous_same_choice_numeric', \n",
    "    'previous_correct_numeric', \n",
    "    # 'previous_port_before_stimulus_numeric', \n",
    "    # 'previous_first_choice_numeric', \n",
    "    'previous_last_choice_numeric', \n",
    "    'time_kernel_impact'\n",
    "    ]\n",
    "df_correctChoice_model_p = pd.DataFrame()\n",
    "df_correctChoice_model_coef = pd.DataFrame()\n",
    "df_correctChoice_model_z =  pd.DataFrame()\n",
    "for df_name, df, color in zip(df_dic_hard_fit_firHalf.keys(), df_dic_hard_fit_firHalf.values(), sns.color_palette(\"colorblind\", len(df_dic_hard_fit_firHalf))):\n",
    "    df_for_fit = dft.parameters_for_fit(df)\n",
    "    df_for_fit = dft.get_time_kernel_impact(df_for_fit, y='correct_numeric', max_lag=19, tau=7)\n",
    "    _, model = utils.logi_model_fit(df_for_fit, X=X, y='correct_numeric')\n",
    "    # plot the results\n",
    "    df_correctChoice_model_p[df_name] = model.pvalues\n",
    "    df_correctChoice_model_coef[df_name] = model.params\n",
    "    df_correctChoice_model_z[df_name] = model.tvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the p-values coefficients and z\n",
    "fig, ax = plt.subplots(3, 1, figsize=(15, 10))\n",
    "df_correctChoice_model_p.index = ['intercept'] + X\n",
    "df_correctChoice_model_coef.index = ['intercept'] + X\n",
    "df_correctChoice_model_z.index = ['intercept'] + X\n",
    "df_correctChoice_model_p_long = df_correctChoice_model_p.reset_index().melt(id_vars='index', var_name='Mouse', value_name='Value')\n",
    "sns.boxplot(data=df_correctChoice_model_p_long, x='index', y='Value', ax=ax[0], color='lightgrey', boxprops=dict(alpha=0.3))\n",
    "sns.scatterplot(data=df_correctChoice_model_p_long, ax=ax[0], x='index', y='Value', hue='Mouse', palette='colorblind')\n",
    "ax[0].axhline(y=0.05, color='red', linestyle='--', label='Significance Threshold (0.05)')\n",
    "df_correctChoice_model_coef_long = df_correctChoice_model_coef.reset_index().melt(id_vars='index', var_name='Mouse', value_name='Value')\n",
    "sns.boxplot(data=df_correctChoice_model_coef_long, x='index', y='Value', ax=ax[1], color='lightgrey', boxprops=dict(alpha=0.3))\n",
    "sns.scatterplot(data=df_correctChoice_model_coef_long, ax=ax[1], x='index', y='Value', hue='Mouse', palette='colorblind')\n",
    "df_correctChoice_model_z_long = df_correctChoice_model_z.reset_index().melt(id_vars='index', var_name='Mouse', value_name='Value')\n",
    "sns.boxplot(data=df_correctChoice_model_z_long, x='index', y='Value', ax=ax[2], color='lightgrey', boxprops=dict(alpha=0.3))\n",
    "sns.scatterplot(data=df_correctChoice_model_z_long, ax=ax[2], x='index', y='Value', hue='Mouse', palette='colorblind')\n",
    "# make x label not overlapping\n",
    "ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=8, ha='right')\n",
    "ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=8, ha='right')\n",
    "ax[2].set_xticklabels(ax[2].get_xticklabels(), rotation=8, ha='right')\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[1].set_xlabel(\"\")\n",
    "ax[2].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"P-values\")\n",
    "ax[1].set_ylabel(\"Coefficients\")\n",
    "ax[2].set_ylabel(\"Z-scores\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "timebin evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timebin_evidence_df = pd.DataFrame()\n",
    "for df_name, df, color in zip(df_dic_hard_aud_fit.keys(), df_dic_hard_aud_fit.values(), sns.color_palette(\"colorblind\", len(df_dic_hard_aud_fit))):\n",
    "    X = np.array([utils.get_timebin_evidence(eval(t)) for t in df['auditory_stimulus']])\n",
    "    y = df['first_choice_numeric']\n",
    "    X_model = sm.add_constant(X) \n",
    "    glm = sm.Logit(y, X_model).fit()\n",
    "    timebin_evidence_df[df_name] = glm.params[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "timebin_evidence_df.index = range(len(timebin_evidence_df))\n",
    "for df_name, col, color in zip(timebin_evidence_df.columns, timebin_evidence_df, sns.color_palette(\"colorblind\", len(timebin_evidence_df))):\n",
    "    plt.plot(timebin_evidence_df[col], color=color, label=df_name, linestyle='--', alpha=0.7)\n",
    "plt.plot(timebin_evidence_df.mean(axis=1), color='black', label='Mean Coefficient', linewidth=2)\n",
    "plt.xlabel(\"Time Bin\")\n",
    "plt.ylabel(\"Coefficient\")\n",
    "plt.title(\"Time Bin Evidence Coefficients for Left Choice\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timebin_evidence_df = pd.DataFrame()\n",
    "for df_name, df, color in zip(df_dic_hard_aud_fit.keys(), df_dic_hard_aud_fit.values(), sns.color_palette(\"colorblind\", len(df_dic_hard_aud_fit))):\n",
    "    X = np.abs(np.array([utils.get_timebin_evidence(eval(t)) for t in df['auditory_stimulus']]))\n",
    "    y = df['correct_numeric']\n",
    "    X_model = sm.add_constant(X) \n",
    "    glm = sm.Logit(y, X_model).fit()\n",
    "    timebin_evidence_df[df_name] = glm.params[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "timebin_evidence_df.index = range(len(timebin_evidence_df))\n",
    "for df_name, col, color in zip(timebin_evidence_df.columns, timebin_evidence_df, sns.color_palette(\"colorblind\", len(timebin_evidence_df))):\n",
    "    plt.plot(timebin_evidence_df[col], color=color, label=df_name, linestyle='--', alpha=0.7)\n",
    "plt.plot(timebin_evidence_df.mean(axis=1), color='black', label='Mean Coefficient', linewidth=2)\n",
    "plt.xlabel(\"Time Bin\")\n",
    "plt.ylabel(\"Coefficient\")\n",
    "plt.title(\"Time Bin Evidence Coefficients for Correct Choice\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_dict = utils.verify_params_time_kernel(dic = df_dic_hard_aud, y='first_choice_numeric')\n",
    "sorted_items = sorted(comb_dict.items(), key=lambda item: abs(item[1]), reverse=True)\n",
    "sorted_items[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['total_percentage_of_tones_left',\n",
    "    'number_of_tones_left',\n",
    "    'percentage_of_timebins_with_evidence_left', \n",
    "    'total_evidence_strength', \n",
    "    'amplitude_strength',\n",
    "    'previous_port_before_stimulus_numeric',\n",
    "    'previous_left_choice_correct_numeric',\n",
    "    'previous_right_choice_wrong_numeric',\n",
    "    'previous_first_choice_numeric', \n",
    "    'previous_last_choice_numeric', \n",
    "    'time_kernel_impact'\n",
    "        ]\n",
    "corr_mat_list, norm_contribution_df = utils.filter_variables_for_model(dic=df_dic_hard_aud, X = X, y='first_choice_numeric', max_lag=2, tau=1)\n",
    "plots.plot_filter_model_variables(corr_mat_list=corr_mat_list, norm_contribution_df=norm_contribution_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\n",
    "    # 'total_percentage_of_tones_left',\n",
    "    # 'number_of_tones_left',\n",
    "    # 'percentage_of_timebins_with_evidence_left', \n",
    "    'total_evidence_strength', \n",
    "    'amplitude_strength',\n",
    "    'previous_port_before_stimulus_numeric',\n",
    "    # 'previous_left_choice_correct_numeric',\n",
    "    'previous_right_choice_wrong_numeric',\n",
    "    'previous_first_choice_numeric', \n",
    "    'previous_last_choice_numeric', \n",
    "    # 'time_kernel_impact'\n",
    "    ]\n",
    "df_leftChoice_model_aud_p = pd.DataFrame()\n",
    "df_leftChoice_model_aud_coef = pd.DataFrame()\n",
    "df_leftChoice_model_aud_z =  pd.DataFrame()\n",
    "for df_name, df, color in zip(df_dic_hard_aud.keys(), df_dic_hard_aud.values(), sns.color_palette(\"colorblind\", len(df_dic_hard_aud))):\n",
    "    df_for_fit = dft.parameters_for_fit(df)\n",
    "    df_for_fit = dft.get_time_kernel_impact(df_for_fit, y='first_choice_numeric', max_lag=2, tau=1)\n",
    "    _, model = utils.logi_model_fit(df_for_fit, X=X, y='first_choice_numeric')\n",
    "    # plot the results\n",
    "    df_leftChoice_model_aud_p[df_name] = model.pvalues\n",
    "    df_leftChoice_model_aud_coef[df_name] = model.params\n",
    "    df_leftChoice_model_aud_z[df_name] = model.tvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the p-values coefficients and z\n",
    "fig, ax = plt.subplots(3, 1, figsize=(15, 10))\n",
    "df_leftChoice_model_aud_p.index = ['intercept'] + X\n",
    "df_leftChoice_model_aud_coef.index = ['intercept'] + X\n",
    "df_leftChoice_model_aud_z.index = ['intercept'] + X\n",
    "df_leftChoice_model_aud_p_long = df_leftChoice_model_aud_p.reset_index().melt(id_vars='index', var_name='Mouse', value_name='Value')\n",
    "sns.boxplot(data=df_leftChoice_model_aud_p_long, x='index', y='Value', ax=ax[0], color='lightgrey', boxprops=dict(alpha=0.3))\n",
    "sns.scatterplot(data=df_leftChoice_model_aud_p_long, ax=ax[0], x='index', y='Value', hue='Mouse', palette='colorblind')\n",
    "ax[0].axhline(y=0.05, color='red', linestyle='--', label='Significance Threshold (0.05)')\n",
    "df_leftChoice_model_aud_coef_long = df_leftChoice_model_aud_coef.reset_index().melt(id_vars='index', var_name='Mouse', value_name='Value')\n",
    "sns.boxplot(data=df_leftChoice_model_aud_coef_long, x='index', y='Value', ax=ax[1], color='lightgrey', boxprops=dict(alpha=0.3))\n",
    "sns.scatterplot(data=df_leftChoice_model_aud_coef_long, ax=ax[1], x='index', y='Value', hue='Mouse', palette='colorblind')\n",
    "df_leftChoice_model_aud_z_long = df_leftChoice_model_aud_z.reset_index().melt(id_vars='index', var_name='Mouse', value_name='Value')\n",
    "sns.boxplot(data=df_leftChoice_model_aud_z_long, x='index', y='Value', ax=ax[2], color='lightgrey', boxprops=dict(alpha=0.3))\n",
    "sns.scatterplot(data=df_leftChoice_model_aud_z_long, ax=ax[2], x='index', y='Value', hue='Mouse', palette='colorblind')\n",
    "# make x label not overlapping\n",
    "ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=8, ha='right')\n",
    "ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=8, ha='right')\n",
    "ax[2].set_xticklabels(ax[2].get_xticklabels(), rotation=8, ha='right')\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[1].set_xlabel(\"\")\n",
    "ax[2].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"P-values\")\n",
    "ax[1].set_ylabel(\"Coefficients\")\n",
    "ax[2].set_ylabel(\"Z-scores\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model for auditory stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(20, 10))\n",
    "for df_name, df, n in zip(df_dic_hard.keys(), df_dic_hard.values(), range(len(df_dic_hard))):\n",
    "    row = n // 5\n",
    "    col = n % 5\n",
    "    for session, color in zip(df['session'].unique(), sns.color_palette(\"crest\", len(df['session'].unique()))):\n",
    "        df_session = df[df['session'] == session]\n",
    "        plots.psychometric_plot(df_session, x='visual_stimulus_ratio', y='left_choice',ax=ax[row, col], point_kwargs={'color': color, 'label' : ''}, line_kwargs={'color': color, 'label': ''})\n",
    "    ax[row, col].set_title(f\"Psychometric Curve for {df_name}\")\n",
    "plt.tight_layout()\n",
    "# Add a colorbar to indicate the session\n",
    "cbar = plt.colorbar(plt.cm.ScalarMappable(cmap=sns.color_palette(\"crest\", as_cmap=True)), orientation='horizontal', ax=ax, shrink=0.3)\n",
    "cbar.set_ticks([])\n",
    "cbar.set_label('before → after')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "for df_name, df, color in zip(df_dic_hard.keys(), df_dic_hard.values(), sns.color_palette(\"colorblind\", len(df_dic_hard))):\n",
    "    # divide into different groups with 1000 trials\n",
    "    df['trial_group'] = np.arange(len(df)) // 1000\n",
    "    lapse_left = []\n",
    "    lapse_right = []\n",
    "    slope = []\n",
    "    bias = []\n",
    "    for group in df['trial_group'].unique():\n",
    "        df_group = df[df['trial_group'] == group]\n",
    "        pleft, params = utils.fit_lapse_logistic_independent(df_group['visual_stimulus_diff'], df_group['left_choice'])\n",
    "        # params in fit_lapse_logistic_independent = (lapse_left, lapse_right, beta, x0)\n",
    "        lapse_left.append(params[0])\n",
    "        lapse_right.append(params[1])\n",
    "        slope.append(params[2])\n",
    "        bias.append(params[3])\n",
    "    ax[0, 0].plot(lapse_left, c=color, label=df_name)\n",
    "    ax[0, 1].plot(lapse_right, c=color, label=df_name)\n",
    "    ax[1, 0].plot(slope, c=color, label=df_name)\n",
    "    ax[1, 1].plot(bias, c=color, label=df_name)\n",
    "ax[0, 0].set_ylabel(\"Lapse Left\")\n",
    "ax[0, 1].set_ylabel(\"Lapse Right\")\n",
    "ax[1, 0].set_ylabel(\"slope\")\n",
    "ax[1, 1].set_ylabel(\"bias\")\n",
    "for ax1 in ax.flat:\n",
    "    ax1.set_xlabel('1000_trials')\n",
    "    ax1.legend()\n",
    "plt.suptitle(\"Model for left choice on visual stimulus ratio\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "for df_name, df, color in zip(df_dic_hard_aud.keys(), df_dic_hard_aud.values(), sns.color_palette(\"colorblind\", len(df_dic_hard_aud))):\n",
    "    # divide into different groups with 1000 trials\n",
    "    df['trial_group'] = np.arange(len(df)) // 1000\n",
    "    lapse_left = []\n",
    "    lapse_right = []\n",
    "    slope = []\n",
    "    bias = []\n",
    "    for group in df['trial_group'].unique():\n",
    "        df_group = df[df['trial_group'] == group]\n",
    "        pleft, params = utils.fit_lapse_logistic_independent(df_group['total_evidence_strength'], df_group['first_choice_numeric'])\n",
    "        # params in fit_lapse_logistic_independent = (lapse_left, lapse_right, beta, x0)\n",
    "        lapse_left.append(params[0])\n",
    "        lapse_right.append(params[1])\n",
    "        slope.append(params[2])\n",
    "        bias.append(params[3])\n",
    "    ax[0, 0].plot(lapse_left, c=color, label=df_name)\n",
    "    ax[0, 1].plot(lapse_right, c=color, label=df_name)\n",
    "    ax[1, 0].plot(slope, c=color, label=df_name)\n",
    "    ax[1, 1].plot(bias, c=color, label=df_name)\n",
    "ax[0, 0].set_ylabel(\"Lapse Left\")\n",
    "ax[0, 1].set_ylabel(\"Lapse Right\")\n",
    "ax[1, 0].set_ylabel(\"slope\")\n",
    "ax[1, 1].set_ylabel(\"bias\")\n",
    "for ax1 in ax.flat:\n",
    "    ax1.set_xlabel('1000_trials')\n",
    "    ax1.legend()\n",
    "plt.suptitle(\"Model for left choice on auditory evidence strenth\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['visual_stimulus_ratio',\n",
    "    'previous_port_before_stimulus_numeric',\n",
    "    'visual_ratio_diff_interact',\n",
    "    'previous_left_choice_correct_numeric',\n",
    "    'previous_right_choice_wrong_numeric',\n",
    "    'previous_first_choice_numeric', \n",
    "    'visual_ratio_bright_interact', \n",
    "    'previous_last_choice_numeric'\n",
    "        ]\n",
    "dic_leftChoice_model_p = {}\n",
    "dic_leftChoice_model_coef = {}\n",
    "dic_leftChoice_model_z = {}\n",
    "for df_name, df, n in zip(df_dic_hard.keys(), df_dic_hard.values(), range(len(df_dic_hard))):\n",
    "    df_leftChoice_model_p = pd.DataFrame()\n",
    "    df_leftChoice_model_coef = pd.DataFrame()\n",
    "    df_leftChoice_model_z =  pd.DataFrame()\n",
    "    # create a session group column\n",
    "    unique_sessions = df['session'].unique()\n",
    "    session_to_group = {session: i // 10 for i, session in enumerate(unique_sessions)} # this groups sessions in groups of 10\n",
    "    df['session_group'] = df['session'].map(session_to_group)\n",
    "    for session, color in zip(df['session_group'].unique(), sns.color_palette(\"crest\", len(df['session'].unique()))):\n",
    "        df_session = df[df['session_group'] == session]\n",
    "        df_session_for_fit = dft.parameters_for_fit(df_session)\n",
    "        # # normalize the X columns in the df_session_for_fit by zscore\n",
    "        # df_session_for_fit[X] = df_session_for_fit[X].apply(lambda x: (x - x.mean()) / x.std())\n",
    "        _, model = utils.logi_model_fit(df_session_for_fit, X=X, y='first_choice_numeric', method='powell')\n",
    "        df_leftChoice_model_p[session] = model.pvalues\n",
    "        df_leftChoice_model_coef[session] = model.params\n",
    "        df_leftChoice_model_z[session] = model.tvalues\n",
    "    df_leftChoice_model_p.index = ['intercept'] + X\n",
    "    df_leftChoice_model_coef.index = ['intercept'] + X\n",
    "    df_leftChoice_model_z.index = ['intercept'] + X\n",
    "    dic_leftChoice_model_p[df_name] = df_leftChoice_model_p.T\n",
    "    dic_leftChoice_model_coef[df_name] = df_leftChoice_model_coef.T\n",
    "    dic_leftChoice_model_z[df_name] = df_leftChoice_model_z.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, len(X)+1, figsize=(30, 15))\n",
    "for n, param in zip(range(len(X)+1), ['intercept'] + X):\n",
    "    for mouse, color in zip(dic_leftChoice_model_p, sns.color_palette(\"colorblind\", len(dic_leftChoice_model_p))):\n",
    "        ax[0, n].plot(dic_leftChoice_model_p[mouse].index, dic_leftChoice_model_p[mouse].iloc[:, n], label=mouse, color=color)\n",
    "    ax[0, n].set_title(param)\n",
    "    ax[0, n].set_xlabel(\"10_Session\")\n",
    "    ax[0, n].set_ylabel(\"p-value\")\n",
    "    ax[0, n].axhline(y=0.05, color='red', linestyle='--', label='Significance 0.05')\n",
    "    ax[0, n].legend()\n",
    "for n, param in zip(range(len(X)+1), ['intercept'] + X):\n",
    "    for mouse, color in zip(dic_leftChoice_model_coef, sns.color_palette(\"colorblind\", len(dic_leftChoice_model_coef))):\n",
    "        ax[1, n].plot(dic_leftChoice_model_coef[mouse].index, dic_leftChoice_model_coef[mouse].iloc[:, n], label=mouse, color=color)\n",
    "    ax[1, n].set_title(param)\n",
    "    ax[1, n].set_xlabel(\"10_Session\")\n",
    "    ax[1, n].set_ylabel(\"Coefficient\")\n",
    "    ax[1, n].legend()\n",
    "for n, param in zip(range(len(X)+1), ['intercept'] + X):\n",
    "    for mouse, color in zip(dic_leftChoice_model_z, sns.color_palette(\"colorblind\", len(dic_leftChoice_model_z))):\n",
    "        ax[2, n].plot(dic_leftChoice_model_z[mouse].index, dic_leftChoice_model_z[mouse].iloc[:, n], label=mouse, color=color)\n",
    "    ax[2, n].set_title(param)\n",
    "    ax[2, n].set_xlabel(\"10_Session\")\n",
    "    ax[2, n].set_ylabel(\"Z-score\")\n",
    "    ax[2, n].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['visual_stimulus_ratio',\n",
    "    'wrong_bright', \n",
    "    # 'wrong_bright_zscore',\n",
    "    'previous_same_choice_correct_numeric', \n",
    "    # 'previous_diff_choice_wrong_numeric', \n",
    "    'previous_same_choice_numeric', \n",
    "    'previous_correct_numeric'\n",
    "    'roa_choice_numeric'\n",
    "    ]\n",
    "dic_correctChoice_model_p = {}\n",
    "dic_correctChoice_model_coef = {}\n",
    "dic_correctChoice_model_z = {}\n",
    "for df_name, df, n in zip(df_dic_hard.keys(), df_dic_hard.values(), range(len(df_dic_hard))):\n",
    "    row = n // 5\n",
    "    col = n % 5\n",
    "    df_correctChoice_model_p = pd.DataFrame()\n",
    "    df_correctChoice_model_coef = pd.DataFrame()\n",
    "    df_correctChoice_model_z =  pd.DataFrame()\n",
    "    # create a session group column\n",
    "    unique_sessions = df['session'].unique()\n",
    "    session_to_group = {session: i // 10 for i, session in enumerate(unique_sessions)} # this groups sessions in groups of 10\n",
    "    df['session_group'] = df['session'].map(session_to_group)\n",
    "    for session, color in zip(df['session_group'].unique(), sns.color_palette(\"crest\", len(df['session'].unique()))):\n",
    "        df_session = df[df['session_group'] == session]\n",
    "        df_session_for_fit = dft.parameters_for_fit(df_session)\n",
    "        # # normalize the X columns in the df_session_for_fit by zscore\n",
    "        # df_session_for_fit[X] = df_session_for_fit[X].apply(lambda x: (x - x.mean()) / x.std())\n",
    "        _, model = utils.logi_model_fit(df_session_for_fit, X=X, y='correct_numeric', method='powell')\n",
    "        df_correctChoice_model_p[session] = model.pvalues\n",
    "        df_correctChoice_model_coef[session] = model.params\n",
    "        df_correctChoice_model_z[session] = model.tvalues\n",
    "    df_correctChoice_model_p.index = ['intercept'] + X\n",
    "    df_correctChoice_model_coef.index = ['intercept'] + X\n",
    "    df_correctChoice_model_z.index = ['intercept'] + X\n",
    "    dic_correctChoice_model_p[df_name] = df_correctChoice_model_p.T\n",
    "    dic_correctChoice_model_coef[df_name] = df_correctChoice_model_coef.T\n",
    "    dic_correctChoice_model_z[df_name] = df_correctChoice_model_z.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, len(X)+1, figsize=(30, 15))\n",
    "for n, param in zip(range(len(X)+1), ['intercept'] + X):\n",
    "    for mouse, color in zip(dic_correctChoice_model_p, sns.color_palette(\"colorblind\", len(dic_correctChoice_model_p))):\n",
    "        ax[0, n].plot(dic_correctChoice_model_p[mouse].index, dic_correctChoice_model_p[mouse].iloc[:, n], label=mouse, color=color)\n",
    "    ax[0, n].set_title(param)\n",
    "    ax[0, n].set_xlabel(\"10_Session\")\n",
    "    ax[0, n].set_ylabel(\"p-value\")\n",
    "    ax[0, n].axhline(y=0.05, color='red', linestyle='--', label='Significance 0.05')\n",
    "    ax[0, n].legend()\n",
    "for n, param in zip(range(len(X)+1), ['intercept'] + X):\n",
    "    for mouse, color in zip(dic_correctChoice_model_coef, sns.color_palette(\"colorblind\", len(dic_correctChoice_model_coef))):\n",
    "        ax[1, n].plot(dic_correctChoice_model_coef[mouse].index, dic_correctChoice_model_coef[mouse].iloc[:, n], label=mouse, color=color)\n",
    "    ax[1, n].set_title(param)\n",
    "    ax[1, n].set_xlabel(\"10_Session\")\n",
    "    ax[1, n].set_ylabel(\"Coefficient\")\n",
    "    ax[1, n].legend()\n",
    "for n, param in zip(range(len(X)+1), ['intercept'] + X):\n",
    "    for mouse, color in zip(dic_correctChoice_model_z, sns.color_palette(\"colorblind\", len(dic_correctChoice_model_z))):\n",
    "        ax[2, n].plot(dic_correctChoice_model_z[mouse].index, dic_correctChoice_model_z[mouse].iloc[:, n], label=mouse, color=color)\n",
    "    ax[2, n].set_title(param)\n",
    "    ax[2, n].set_xlabel(\"10_Session\")\n",
    "    ax[2, n].set_ylabel(\"Z-score\")\n",
    "    ax[2, n].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kudongdong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
